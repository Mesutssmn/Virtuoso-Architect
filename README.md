# üéπ Virtuoso Architect

<div align="center">

![License](https://img.shields.io/badge/license-MIT-blue.svg)
![Python](https://img.shields.io/badge/python-3.8%2B-blue)
![Status](https://img.shields.io/badge/status-active-success)

**An AI-Powered System for Quantifying Piano Music Technical Difficulty**

*Transforming subjective musical complexity into objective, data-driven metrics using Machine Learning and Music Information Retrieval*

[Features](#-key-features) ‚Ä¢ [Installation](#-installation) ‚Ä¢ [Usage](#-usage-guide) ‚Ä¢ [Architecture](#-system-architecture) ‚Ä¢ [Contributing](#-contributing)

</div>

---

## üìã Table of Contents

- [Overview](#overview)
- [Key Features](#key-features)
- [Installation](#installation)
- [Usage Guide](#usage-guide)
  - [Feature Extraction System](#feature-extraction)
  - [Labeling Systems](#labeling-systems)
  - [Training Pipeline](#training-pipeline)
  - [Evaluation & Verification](#evaluation-verification)
- [Classification Schema](#classification-schema)
- [System Architecture](#system-architecture)
- [Project Structure](#project-structure)
- [Feature Engineering](#feature-engineering)
- [Auto-Labeling System](#auto-labeling-system)
- [Manual Labeling Interface](#manual-labeling-interface)
- [Training & Model Architecture](#training-model-architecture)
- [Technology Stack](#technology-stack)
- [Future Improvements](#future-improvements)
- [Contributing](#contributing)
- [License](#license)

---

<a id="overview"></a>
## üéØ Overview

**Virtuoso Architect** is a comprehensive, research-grade system designed to objectively quantify the technical difficulty of piano compositions. The project addresses a fundamental challenge in music education and analysis: converting subjective assessments of musical complexity into precise, reproducible metrics.

### The Challenge

Traditional difficulty assessment relies on expert opinion, which is:
- **Subjective**: Varies between evaluators
- **Inconsistent**: Hard to reproduce across pieces
- **Time-consuming**: Requires extensive musical training
- **Non-scalable**: Cannot process large music libraries

### Our Solution

We've built an end-to-end ML pipeline that:

1. **Extracts** musical features from MIDI files (sourced from [GiantMIDI-Piano](https://www.kaggle.com/datasets/pictureinthenoise/music-generation-with-giantmidi-piano)) using Music Information Retrieval (MIR) techniques
2. **Quantifies** technical complexity through 10+ engineered features
3. **Classifies** pieces into difficulty categories using gradient boosting
4. **Validates** predictions through both automated rules and human-in-the-loop verification

The system achieves **professional-grade accuracy** by combining music theory expertise with modern machine learning, making it suitable for:
- üéì Music education platforms
- üìö Digital sheet music libraries
- üî¨ Music AI research
- üéº Composer tools and analysis

---

<a id="key-features"></a>
## ‚ú® Key Features

### üéº Musical Intelligence
- **Advanced Feature Extraction**: 10+ metrics capturing hand stretch, polyphony, rhythm complexity, and more
- **Theory-Driven Design**: Features based on established piano pedagogy and biomechanics
- **MIDI Processing**: Works with standard MIDI files (`.mid`, `.midi`)

### ü§ñ Machine Learning Pipeline
- **XGBoost Classifier**: State-of-the-art gradient boosting optimized for tabular data
- **Dual Classification Schema**: Support for both 4-class and 5-class taxonomies
- **Automated Evaluation**: Confusion matrices, precision/recall metrics, classification reports

### üè∑Ô∏è Flexible Labeling
- **Auto-Labeling**: Rule-based system applying music theory heuristics
- **Manual Interface**: Web-based UI for human verification and correction
- **Hybrid Workflow**: Combine automated speed with human expertise

### üîß Production-Ready
- **Modular Architecture**: Clean separation of concerns (ETL, training, evaluation)
- **Configuration Management**: Centralized config system preventing drift
- **Reproducibility**: Versioned datasets, deterministic splitting, saved models

---

<a id="installation"></a>
## üíª Installation

### Prerequisites

- **Python 3.8+** (tested on 3.8, 3.9, 3.10)
- **pip** package manager
- **Git** (for cloning the repository)

### Step 1: Clone the Repository

```bash
git clone https://github.com/Mesutssmn/Virtuoso-Architect.git
cd Virtuoso-Architect
```

### Step 2: Create a Virtual Environment (Recommended)

```bash
# Using venv
python -m venv venv

# Activate on macOS/Linux
source venv/bin/activate

# Activate on Windows
venv\Scripts\activate
```

### Step 3: Install Dependencies

```bash
pip install -r requirements.txt
```

**Expected installation time**: 2-5 minutes (depending on network speed)

### Step 4: Prepare Data Directories

```bash
# Create necessary directories if they don't exist
mkdir -p data/raw_midi
mkdir -p data/processed/labels
mkdir -p models
```

### Step 5: Add Your MIDI Files

Place your MIDI files (`.mid` or `.midi`) in `data/raw_midi/`:

```bash
cp /path/to/your/midi/files/*.mid data/raw_midi/
```

**Recommendation**: Start with 50-100 files for initial testing, then scale to thousands.

### Step 6: Configure Environment (Optional)

If using OpenAI or other API-based features:

```bash
cp .env.example .env
# Edit .env and add your API keys
nano .env  # or use your preferred editor
```

---

<a id="usage-guide"></a>
## üìñ Usage Guide

### Workflow Overview

```
Extract Features ‚Üí Generate Labels ‚Üí Train Model ‚Üí Evaluate Performance
```

---

<a id="classification-schema"></a>
## üè∑Ô∏è Classification Schema

We support **two taxonomies** for different use cases:

---

### üéØ **4-Class Schema** (Balanced, ML-Optimized)

**Purpose**: Simplified categories with clear boundaries. Optimized for machine learning stability.

| ID | Label Name | Description | Key Indicators |
|----|------------|-------------|----------------|
| **0** | **Far Reach** | Wide hand spans requiring extended finger stretches | `max_stretch > 25` semitones (>2 octaves) |
| **1** | **Double Thirds** | Rapid alternation of Major/Minor 3rd intervals | `thirds_frequency > 0.4` (40% of intervals are thirds) |
| **2** | **Advanced Chords** | Complex chord structures with 4+ simultaneous notes | `avg_density > 4.0` notes at once |
| **3** | **Advanced Counterpoint** | Independent melodic lines creating polyphonic texture | `polyphony_score > 3.0` (3+ distinct voices) |

**Use Cases**:
- Initial model training with balanced datasets
- Fast auto-labeling for large corpora
- Avoiding class imbalance issues

---

### üéº **5-Class Schema** (Granular, Musicologically Rich)

**Purpose**: Captures nuanced difficulty categories. Preferred for music education applications.

| ID | Label Name | Description | Example Pieces |
|----|------------|-------------|----------------|
| **0** | **Far Reach** | Extended hand positions | Rachmaninoff Prelude Op. 23 No. 5 |
| **1** | **Double Thirds** | Repeated thirds patterns | Chopin √âtude Op. 25 No. 6 |
| **2** | **Advanced Chords** | Dense harmonic structures | Liszt Hungarian Rhapsody No. 2 |
| **3** | **Advanced Counterpoint** | Basic fugal/polyphonic writing | Bach 2-Part Inventions |
| **4** | **Multiple Voices** | Complex polyphony - 3+ independent voices (extends Class 3) | Bach Goldberg Variations, 4-voice Fugues |

**New in 5-Class**:
- **Multiple Voices** (ID 4): Distinguishes between basic counterpoint (2 voices) and complex polyphony (3+ voices)
  - Think: Bach 4-voice fugues vs. two-part inventions

**ID Mapping Strategy**:
- IDs 0-3 are **shared** between both schemas (Far Reach, Double Thirds, Advanced Chords, Advanced Counterpoint)
- ID 4 (Multiple Voices) is **exclusive** to 5-class schema
- This ensures **clean separation**: removing ID 4 creates the 4-class schema without renumbering

---

### Schema Selection Guide

```mermaid
graph TD
    A[Choose Schema] --> B{Dataset Size?}
    B -->|< 1000 files| C[Use 4-Class]
    B -->|> 1000 files| D{Application?}
    D -->|ML Research| C
    D -->|Music Education| E[Use 5-Class]
    C --> F[Balanced Classes]
    E --> G[Granular Insights]
```

**Configuration**:

Edit `tools/labeling/config.py`:

```python
# Switch between schemas
ACTIVE_SCHEMA = '5_labels'  # or '4_labels'

LABEL_SCHEMAS = {
    '4_labels': {
        0: 'Far Reach',
        1: 'Double Thirds',
        2: 'Advanced Chords',
        3: 'Advanced Counterpoint'
    },
    '5_labels': {
        0: 'Far Reach',
        1: 'Double Thirds',
        2: 'Advanced Chords',
        3: 'Advanced Counterpoint',
        4: 'Multiple Voices'
    }
}
```

Changes here **automatically propagate** to:
- Auto-labeling rules
- Manual labeling UI (button labels)
- Training scripts (number of classes)
- Evaluation reports

---

<a id="system-architecture"></a>
## üèóÔ∏è System Architecture

### High-Level Data Flow

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Raw MIDI Files ‚îÇ
‚îÇ   (10,000+)     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Feature Extraction Engine  ‚îÇ
‚îÇ  (extract_features.py)      ‚îÇ
‚îÇ                             ‚îÇ
‚îÇ  ‚Ä¢ Parse MIDI events        ‚îÇ
‚îÇ  ‚Ä¢ Calculate statistics     ‚îÇ
‚îÇ  ‚Ä¢ Detect patterns          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   features_all.csv          ‚îÇ
‚îÇ   (Feature Store)           ‚îÇ
‚îÇ                             ‚îÇ
‚îÇ  Columns:                   ‚îÇ
‚îÇ  ‚Ä¢ max_stretch              ‚îÇ
‚îÇ  ‚Ä¢ avg_density              ‚îÇ
‚îÇ  ‚Ä¢ polyphony_score          ‚îÇ
‚îÇ  ‚Ä¢ thirds_frequency         ‚îÇ
‚îÇ  ‚Ä¢ ... (10 total)           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ                          ‚îÇ
         ‚ñº                          ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Auto-Labeling   ‚îÇ      ‚îÇ Manual Labeling  ‚îÇ
‚îÇ  (Rule Engine)   ‚îÇ      ‚îÇ  (Human Verify)  ‚îÇ
‚îÇ                  ‚îÇ      ‚îÇ                  ‚îÇ
‚îÇ  ‚Ä¢ Apply         ‚îÇ      ‚îÇ  ‚Ä¢ Web UI        ‚îÇ
‚îÇ    thresholds    ‚îÇ      ‚îÇ  ‚Ä¢ Keyboard      ‚îÇ
‚îÇ  ‚Ä¢ Music theory  ‚îÇ      ‚îÇ    shortcuts     ‚îÇ
‚îÇ    heuristics    ‚îÇ      ‚îÇ  ‚Ä¢ Listen to     ‚îÇ
‚îÇ                  ‚îÇ      ‚îÇ    MIDI          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ                         ‚îÇ
         ‚ñº                         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         labels/ Directory                ‚îÇ
‚îÇ                                          ‚îÇ
‚îÇ  ‚Ä¢ auto_4_labels.csv                     ‚îÇ
‚îÇ  ‚Ä¢ auto_5_labels.csv                     ‚îÇ
‚îÇ  ‚Ä¢ manual_4_labels.csv                   ‚îÇ
‚îÇ  ‚Ä¢ manual_5_labels.csv                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Training Pipeline         ‚îÇ
‚îÇ   (train_with_labels.py)    ‚îÇ
‚îÇ                             ‚îÇ
‚îÇ  1. Merge features + labels ‚îÇ
‚îÇ  2. Split 80/20             ‚îÇ
‚îÇ  3. Train XGBoost           ‚îÇ
‚îÇ  4. Save model              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Trained Model             ‚îÇ
‚îÇ   (xgb_classifier_5.pkl)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Evaluation                ‚îÇ
‚îÇ   (evaluate_model.py)       ‚îÇ
‚îÇ                             ‚îÇ
‚îÇ  ‚Ä¢ Confusion Matrix         ‚îÇ
‚îÇ  ‚Ä¢ Precision/Recall         ‚îÇ
‚îÇ  ‚Ä¢ Classification Report    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Component Interactions

#### üîÑ **Feature Extraction Pipeline**
```python
# High-level pseudocode
for midi_file in data/raw_midi/:
    midi_data = parse_midi(midi_file)
    
    features = {
        'max_stretch': calculate_hand_stretch(midi_data),
        'max_chord_size': count_max_simultaneous_notes(midi_data),
        'note_density': calculate_notes_per_second(midi_data),
        'poly_voice_count': detect_independent_voices(midi_data),
        'thirds_frequency': count_interval_patterns(midi_data, interval=3),
        # ... 5 more features
    }
    
    write_to_csv('features_all.csv', features)
```

#### üè∑Ô∏è **Labeling Pipeline**
```python
# Auto-labeling (rules-based)
def auto_label(features):
    if features['max_stretch'] > 25:
        return 'Far Reach'
    elif features['thirds_frequency'] > 0.4:
        return 'Double Thirds'
    elif features['max_chord_size'] > 9:
        return 'Advanced Chords'
    elif features['poly_voice_count'] > 3.5:
        return 'Multiple Voices'
    # ... more rules

# Manual labeling (human-in-the-loop)
# User sees feature summary + listens to MIDI
# Presses keyboard 0-4 to assign label
# Saves to manual_X_labels.csv
```

#### ü§ñ **Training Pipeline**
```python
# Load and merge
features = pd.read_csv('features_all.csv')
labels = pd.read_csv('labels/auto_5_labels.csv')
dataset = features.merge(labels, on='filename')

# Split
X_train, X_test, y_train, y_test = train_test_split(
    dataset[feature_columns], 
    dataset['label'], 
    test_size=0.2,
    random_state=42  # Reproducibility
)

# Train
model = xgb.XGBClassifier(
    objective='multi:softmax',
    num_class=5,
    max_depth=6,
    learning_rate=0.1
)
model.fit(X_train, y_train)

# Evaluate
predictions = model.predict(X_test)
accuracy = (predictions == y_test).mean()
```

---

<a id="project-structure"></a>
## üìÇ Project Structure

```
Virtuoso-Architect/
‚îÇ
‚îú‚îÄ‚îÄ üìÅ data/                          # Data Lake & Warehouse
‚îÇ   ‚îú‚îÄ‚îÄ raw_midi/                     # Source of truth: Original MIDI files
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ *.mid, *.midi             # Thousands of piano compositions
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ processed/                    # Transformed data artifacts
‚îÇ       ‚îú‚îÄ‚îÄ features_all.csv          # Feature Store: Extracted metrics for all files
‚îÇ       ‚îî‚îÄ‚îÄ labels/                   # Ground Truth Store
‚îÇ           ‚îú‚îÄ‚îÄ auto_4_labels.csv     # Auto-generated 4-class labels
‚îÇ           ‚îú‚îÄ‚îÄ auto_5_labels.csv     # Auto-generated 5-class labels
‚îÇ           ‚îú‚îÄ‚îÄ manual_4_labels.csv   # Human-verified 4-class labels
‚îÇ           ‚îî‚îÄ‚îÄ manual_5_labels.csv   # Human-verified 5-class labels
‚îÇ
‚îú‚îÄ‚îÄ üìÅ models/                        # Trained Model Artifacts
‚îÇ   ‚îî‚îÄ‚îÄ difficulty_classifier.pkl     # Trained XGBoost classifier (5-class by default)
‚îÇ
‚îú‚îÄ‚îÄ üìÅ scripts/                       # Core ML Pipeline Scripts
‚îÇ   ‚îú‚îÄ‚îÄ extract_features.py           # ETL: MIDI ‚Üí Features (CSV)
‚îÇ   ‚îú‚îÄ‚îÄ train_with_labels.py          # Training: Features + Labels ‚Üí Model
‚îÇ   ‚îú‚îÄ‚îÄ evaluate_model.py             # Evaluation: Model + Test Set ‚Üí Metrics
‚îÇ   ‚îú‚îÄ‚îÄ analyze_model.py              # Analysis: Feature importance & correlations
‚îÇ   ‚îî‚îÄ‚îÄ verify_system.py              # Integration Testing: End-to-End Pipeline
‚îÇ
‚îú‚îÄ‚îÄ üìÅ src/                           # Source Code Modules
‚îÇ   ‚îú‚îÄ‚îÄ main.py                       # üöÄ CLI Entry Point: Single file analysis
‚îÇ   ‚îú‚îÄ‚îÄ ml_engine/                    # Machine Learning Core
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ feature_extract.py        # Logic for converting MIDI to features
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ train.py                  # XGBoost training logic
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ rag_engine/                   # RAG AI Module (Fully Implemented)
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ retriever.py              # GPT-4o connector (requires OPENAI_API_KEY)
‚îÇ       ‚îî‚îÄ‚îÄ knowledge_base.json       # Pedagogy database
‚îÇ
‚îú‚îÄ‚îÄ üìÅ tools/                         # Labeling & Utilities
‚îÇ   ‚îî‚îÄ‚îÄ labeling/                     # Ground Truth Generation Suite
‚îÇ       ‚îú‚îÄ‚îÄ config.py                 # üß† CENTRAL CONFIG: Label schemas & thresholds
‚îÇ       ‚îÇ
‚îÇ       ‚îú‚îÄ‚îÄ auto/                     # Automated Labeling System
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ auto_label.py         # Rule engine applying theory-based heuristics
‚îÇ       ‚îÇ
‚îÇ       ‚îî‚îÄ‚îÄ manual/                   # Human-in-the-Loop Labeling
‚îÇ           ‚îú‚îÄ‚îÄ labeling_server.py    # Flask API serving features & MIDI info
‚îÇ           ‚îú‚îÄ‚îÄ labeling_interface.html # Single-page web UI for annotation
‚îÇ           ‚îî‚îÄ‚îÄ start_labeling.py     # Launcher script (starts server + opens browser)
‚îÇ
‚îú‚îÄ‚îÄ üìÑ .env.example                   # Template for environment variables
‚îú‚îÄ‚îÄ üìÑ .gitignore                     # Excludes large files, venv, cache
‚îú‚îÄ‚îÄ üìÑ LICENSE                        # MIT License
‚îú‚îÄ‚îÄ üìÑ README.md                      # This file
‚îî‚îÄ‚îÄ üìÑ requirements.txt               # Python dependencies
```

### Directory Philosophy

#### üóÑÔ∏è **Data Layer** (`data/`)
- **Separation of Concerns**: Raw MIDI files remain immutable; processed features are derived artifacts
- **Versioning**: Different label versions coexist (e.g., `auto_4_labels.csv` vs `manual_5_labels.csv`)
- **Reproducibility**: Any experiment can be recreated by referencing specific label files

#### üß† **Logic Layer** (`scripts/` & `src/`)
- **Scripts**: High-level workflows (extract ‚Üí train ‚Üí evaluate)
- **Modules**: Reusable components (feature extraction algorithms, model wrappers)
- **Single Responsibility**: Each script has one clear purpose

#### üîß **Tooling Layer** (`tools/`)
- **Labeling First**: Ground truth is critical; dedicated tools ensure quality
- **Config-Driven**: All labeling logic pulls from `config.py` to prevent schema drift

---

<a id="feature-engineering"></a>
## üî¨ Feature Engineering

Our feature set is **theory-driven** and **biomechanically motivated**. Each feature captures a specific aspect of piano technique difficulty.

---

### Feature Catalog

| # | Feature Name | Type | Range | Description |
|---|--------------|------|-------|-------------|
| 1 | **max_stretch** | Float | 0-48 | Maximum interval (in semitones) between lowest and highest notes played simultaneously. Values: 12=octave, 24=2 octaves, >25=difficult reach |
| 2 | **max_chord_size** | Integer | 1-15+ | Maximum number of notes sounding simultaneously |
| 3 | **note_density** | Float | 1-30+ | Average notes per second. Measures technical speed and density |
| 4 | **left_hand_activity** | Float | 0-1 | Ratio of left hand notes (below middle C) to total notes |
| 5 | **avg_tempo** | Float | 40-200 | Average tempo in BPM (beats per minute) extracted from MIDI |
| 6 | **dynamic_range** | Float | 0-127 | Range of note velocities (difference between softest and loudest notes) |
| 7 | **poly_voice_count** | Float | 1-6+ | Average number of simultaneous independent melodic voices |
| 8 | **octave_jump_frequency** | Float | 0-1 | Proportion of melodic intervals that are octave jumps or larger |
| 9 | **thirds_frequency** | Float | 0-1 | Proportion of melodic intervals that are Major or Minor thirds. High values (>0.4) indicate technical etudes |
| 10 | **polyrhythm_score** | Float | 0-1 | Measure of rhythmic complexity and polyrhythmic patterns |

---

### Feature Extraction Deep Dive

#### 1Ô∏è‚É£ **Max Stretch** (Hand Span)

**Biomechanical Basis**: Human hands can comfortably span ~1.5 octaves. Beyond 2 octaves requires:
- Extended finger positions
- Wrist rotation
- Exceptional hand size

**Calculation**:
```python
def calculate_max_stretch(midi):
    max_stretch = 0
    for timestep in midi.get_piano_roll():
        active_notes = get_active_notes(timestep)
        if len(active_notes) >= 2:
            stretch = max(active_notes) - min(active_notes)
            max_stretch = max(max_stretch, stretch)
    return max_stretch
```

**Example Values**:
- 12 (octave): Beginner level
- 18 (octave + fifth): Intermediate
- 24 (2 octaves): Advanced
- >28: Virtuoso (rare, Rachmaninoff-level)

---

#### 2Ô∏è‚É£ **Polyphony Score** (Voice Independence)

**Music Theory Basis**: Polyphony = multiple independent melodic lines. Unlike "note density" (just counting notes), this measures:
- Distinct pitch contours
- Rhythmic independence
- Harmonic coherence

**Calculation** (using `music21`):
```python
def calculate_polyphony(midi):
    score = music21.converter.parse(midi)
    voices = score.parts  # Separate staves/hands
    
    independent_voices = 0
    for voice in voices:
        # Check if voice has distinct melodic contour
        if has_distinct_melody(voice):
            independent_voices += 1
    
    return independent_voices
```

**Example Scores**:
- 1.0: Monophonic (single melody)
- 2.0: Melody + accompaniment
- 3.0: Basic counterpoint (Bach 2-part inventions)
- 4.0+: Complex polyphony (Bach fugues)

---

#### 3Ô∏è‚É£ **Thirds Frequency** (Pattern Recognition)

**Pedagogical Motivation**: Scales in thirds are a **specific technique** taught in conservatories.

**Calculation**:
```python
def calculate_thirds_frequency(midi):
    intervals = get_melodic_intervals(midi)  # [3, 5, 3, 2, 3, ...]
    thirds = [i for i in intervals if i in [3, 4]]  # Major/Minor thirds
    return len(thirds) / len(intervals)
```

**Interpretation**:
- <0.2: Occasional thirds
- 0.2-0.4: Moderate use
- >0.4: **Dedicated thirds etude** (e.g., Chopin Op. 25 No. 6)

---

### Feature Importance (Post-Training Analysis)

After training, we can inspect which features contribute most to predictions:

```python
import xgboost as xgb

model = xgb.Booster()
model.load_model('models/xgb_classifier_5.pkl')

importance = model.get_score(importance_type='gain')
# Output:
# {
#     'max_stretch': 0.35,       # 35% of decision power
#     'polyphony_score': 0.28,
#     'avg_density': 0.18,
#     'thirds_frequency': 0.12,
#     ...
# }
```

**Insight**: `max_stretch` and `polyphony_score` dominate predictions ‚Üí Physical difficulty and textural complexity are the primary difficulty factors.

---

<a id="auto-labeling-system"></a>
## ü§ñ Auto-Labeling System

### Design Philosophy

**Goal**: Apply **music theory expertise** at scale without human intervention.

**Challenges**:
1. Rules must be **precise** to avoid mislabeling
2. Edge cases (e.g., sparse chords vs. dense melodies) need handling
3. Must remain **interpretable** (no black-box heuristics)

---

### Rule Engine Architecture

**File**: `tools/labeling/auto/auto_label.py`

#### Step 1: Load Feature Vector

```python
features = pd.read_csv('data/processed/features_all.csv')
# Shape: (10247, 10)  # 10,247 files √ó 10 features
```

---

#### Step 2: Apply Decision Tree

```python
def auto_label_5_class(row):
    # Rule 1: Far Reach (biomechanical constraint)
    if row['max_stretch'] > 25:
        return 0  # Far Reach
    
    # Rule 2: Double Thirds (pattern-based)
    if row['thirds_frequency'] > 0.4:
        return 1  # Double Thirds
    
    # Rule 3: Advanced Chords (density-based)
    if row['avg_density'] > 4.0 and row['max_stretch'] > 15:
        return 2  # Advanced Chords
    
    # Rule 4: Multiple Voices (polyphony-based)
    if row['polyphony_score'] > 4.0:
        return 4  # Multiple Voices (5-class only)
    
    # Rule 5: Advanced Counterpoint (default for polyphonic)
    if row['polyphony_score'] > 2.5:
        return 3  # Advanced Counterpoint
    
    # Fallback
    return 3  # Default to Counterpoint
```

---

#### Step 3: Batch Processing

```python
labels = features.apply(auto_label_5_class, axis=1)

# Save to CSV
output = pd.DataFrame({
    'filename': features['filename'],
    'label_id': labels
})
output.to_csv('data/processed/labels/auto_5_labels.csv', index=False)
```

**Performance**: 10,000 files labeled in **~5 seconds** on modern hardware.

---

### Configuration System

**File**: `tools/labeling/config.py`

**Central Control**: All thresholds defined here to prevent hardcoding.

```python
# Auto-labeling thresholds (5-class schema)
AUTO_LABEL_THRESHOLDS_5 = {
    'far_reach': {
        'conditions': {
            'max_stretch': {'operator': '>', 'value': 25}
        },
        'label_id': 0
    },
    'double_thirds': {
        'conditions': {
            'thirds_frequency': {'operator': '>', 'value': 0.4}
        },
        'label_id': 1
    },
    'advanced_chords': {
        'conditions': {
            'avg_density': {'operator': '>', 'value': 4.0},
            'max_stretch': {'operator': '>', 'value': 15}
        },
        'logic': 'AND',  # Both conditions must be true
        'label_id': 2
    },
    # ... more rules
}
```

**Benefits**:
1. **Auditable**: Non-programmers can review rules
2. **Tunable**: Adjust thresholds without touching code
3. **Consistent**: Same rules used in reporting and training

---

### Validation & Quality Assurance

After auto-labeling, we can run **sanity checks** to validate label distribution and detect edge cases. (Note: A validation script can be implemented in `tools/labeling/auto/` for this purpose.)

**Output**:
```
Validation Report for auto_5_labels.csv
========================================

Class Distribution:
  Far Reach: 2,134 (20.8%)
  Double Thirds: 1,892 (18.5%)
  Advanced Chords: 3,021 (29.5%)
  Advanced Counterpoint: 2,456 (24.0%)
  Multiple Voices: 744 (7.3%)

‚ö† Warning: Class imbalance detected
  - "Multiple Voices" is underrepresented (< 10%)
  - Recommendation: Collect more polyphonic pieces or merge with Class 3

Edge Cases Detected:
  - 42 files have max_stretch > 30 but low density
    ‚Üí May be sparse wide-interval passages, not chords
  - 18 files labeled "Double Thirds" but thirds_freq < 0.35
    ‚Üí Near threshold; recommend manual review

‚úì No files with invalid label IDs
‚úì All filenames exist in features_all.csv
```

---

<a id="manual-labeling-interface"></a>
## üñ•Ô∏è Manual Labeling Interface

### Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Browser (Client)   ‚îÇ
‚îÇ  labeling_interface ‚îÇ
‚îÇ       .html         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ HTTP Requests
           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Flask Server       ‚îÇ
‚îÇ  labeling_server.py ‚îÇ
‚îÇ                     ‚îÇ
‚îÇ  Endpoints:         ‚îÇ
‚îÇ  /api/config        ‚îÇ
‚îÇ  /api/next_file     ‚îÇ
‚îÇ  /api/save_label    ‚îÇ
‚îÇ  /api/features      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ File I/O
           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Filesystem         ‚îÇ
‚îÇ  ‚Ä¢ features.csv     ‚îÇ
‚îÇ  ‚Ä¢ labels.csv       ‚îÇ
‚îÇ  ‚Ä¢ raw_midi/        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

### Backend API (Flask)

**File**: `tools/labeling/manual/labeling_server.py`

#### Endpoint: `GET /api/config`

**Purpose**: Dynamically load label schema from `config.py`

**Response**:
```json
{
  "schema": "5_labels",
  "labels": {
    "0": "Far Reach",
    "1": "Double Thirds",
    "2": "Advanced Chords",
    "3": "Advanced Counterpoint",
    "4": "Multiple Voices"
  }
}
```

**Why This Matters**: If you change label names in `config.py`, the UI updates **automatically** without touching JavaScript.

---

#### Endpoint: `GET /api/next_file`

**Purpose**: Serve the next unlabeled file

**Response**:
```json
{
  "filename": "chopin_op10_no1.mid",
  "features": {
    "max_stretch": 32.0,
    "avg_density": 4.5,
    "polyphony_score": 2.1,
    "thirds_frequency": 0.65
  },
  "auto_suggestion": 1,  // Auto-labeler suggests "Double Thirds"
  "midi_data_url": "/data/raw_midi/chopin_op10_no1.mid",
  "progress": {
    "labeled": 234,
    "total": 10247
  }
}
```

---

#### Endpoint: `POST /api/save_label`

**Purpose**: Save human label to CSV

**Request**:
```json
{
  "filename": "chopin_op10_no1.mid",
  "label_id": 1,
  "schema": "5_labels"
}
```

**Response**:
```json
{
  "status": "success",
  "message": "Label saved to manual_5_labels.csv"
}
```

**Implementation** (Simplified):
```python
@app.route('/api/save_label', methods=['POST'])
def save_label():
    data = request.json
    filename = data['filename']
    label_id = data['label_id']
    schema = data['schema']
    
    # Append to CSV
    label_file = f'data/processed/labels/manual_{schema}.csv'
    with open(label_file, 'a') as f:
        f.write(f"{filename},{label_id}\n")
    
    return {'status': 'success'}
```

---

### Frontend (Single-Page Application)

**File**: `tools/labeling/manual/labeling_interface.html`

**Tech Stack**: Vanilla JavaScript + HTML5 + CSS3 (no frameworks)

**Why No React/Vue?**
- Simplicity: No build step, works immediately
- Compatibility: Runs on any browser
- Low latency: Direct DOM manipulation

#### Key Features

##### 1Ô∏è‚É£ **Dynamic Label Buttons**

```javascript
// Fetch config from backend
fetch('/api/config')
  .then(res => res.json())
  .then(config => {
    const buttonContainer = document.getElementById('label-buttons');
    
    // Generate buttons dynamically
    Object.entries(config.labels).forEach(([id, name]) => {
      const btn = document.createElement('button');
      btn.textContent = `[${id}] ${name}`;
      btn.onclick = () => saveLabel(id);
      buttonContainer.appendChild(btn);
    });
  });
```

**Effect**: Changing `config.py` from 4 to 5 labels re-renders the UI **without code changes**.

---

##### 2Ô∏è‚É£ **MIDI Playback**

```javascript
// Use Web MIDI API (if available) or fallback to audio conversion
function playMIDI(filepath) {
  const audio = new Audio(`/convert_to_audio?file=${filepath}`);
  audio.play();
}
```

**Note**: We provide a fallback endpoint that converts MIDI ‚Üí WAV using `pretty_midi.fluidsynth`.

---

##### 3Ô∏è‚É£ **Keyboard Shortcuts**

```javascript
document.addEventListener('keydown', (e) => {
  if (e.key >= '0' && e.key <= '4') {
    saveLabel(parseInt(e.key));
  }
  if (e.key === 'ArrowRight') loadNextFile();
  if (e.key === 'ArrowLeft') loadPreviousFile();
  if (e.key === ' ') togglePlayback();
});
```

**UX Goal**: Expert labelers can annotate **100+ files/hour** without touching the mouse.

---

##### 4Ô∏è‚É£ **Feature Visualization**

```javascript
function displayFeatures(features) {
  const container = document.getElementById('features');
  
  // Bar chart using Chart.js (included via CDN)
  new Chart(container, {
    type: 'bar',
    data: {
      labels: Object.keys(features),
      datasets: [{
        data: Object.values(features),
        backgroundColor: '#3498db'
      }]
    }
  });
}
```

**Output**:
```
Max Stretch    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 32.0
Avg Density    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 4.5
Polyphony      ‚ñà‚ñà‚ñà‚ñà 2.1
Thirds Freq    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 0.65
```

---

### Launching the Interface

```bash
# Start server
python tools/labeling/manual/start_labeling.py

# Output:
# Starting Flask server on http://localhost:5000...
# Opening browser...
# Server is ready! Press Ctrl+C to stop.
```

**Auto-Opens**: The script uses `webbrowser` module to launch your default browser automatically.

---

<a id="training-model-architecture"></a>
## üß† Training & Model Architecture

### Why XGBoost?

**Decision Matrix**:

| Model Type | Pros | Cons | Verdict |
|------------|------|------|---------|
| **Neural Network** | Flexible, can learn complex patterns | Needs massive data (100k+ samples), black-box | ‚ùå Overkill for tabular data |
| **Random Forest** | Good baseline, interpretable | Slower than XGBoost, less accurate | ‚ö†Ô∏è Viable alternative |
| **XGBoost** | Fast, accurate, handles imbalance, feature importance | Requires tuning | ‚úÖ **Chosen** |
| **SVM** | Works well for small datasets | Struggles with multi-class, slow | ‚ùå Not scalable |

**XGBoost Advantages**:
1. **Tabular data specialist**: Our features are structured (not images/audio)
2. **Built-in regularization**: Prevents overfitting with small datasets
3. **Parallel training**: Fast on multi-core CPUs
4. **Interpretability**: Feature importance, tree visualization

---

### Training Pipeline Breakdown

**File**: `scripts/train_with_labels.py`

#### Step 1: Data Loading

```python
import pandas as pd

features = pd.read_csv('data/processed/features_all.csv')
labels = pd.read_csv('data/processed/labels/auto_5_labels.csv')

# Merge on filename
dataset = pd.merge(features, labels, on='filename')
```

---

#### Step 2: Train/Test Split

```python
from sklearn.model_selection import train_test_split

X = dataset[FEATURE_COLUMNS]  # 10 features
y = dataset['label_id']       # 0-4

X_train, X_test, y_train, y_test = train_test_split(
    X, y, 
    test_size=0.2,      # 80/20 split
    random_state=42,    # Reproducibility
    stratify=y          # Preserve class distribution
)
```

**Stratification**: Ensures test set has same class proportions as training set.

---

#### Step 3: Model Initialization

```python
import xgboost as xgb

model = xgb.XGBClassifier(
    objective='multi:softmax',  # Multi-class classification
    num_class=5,                # Number of labels
    max_depth=6,                # Tree depth
    learning_rate=0.1,          # Gradient descent step size
    n_estimators=100,           # Number of boosting rounds
    subsample=0.8,              # Row sampling (prevents overfitting)
    colsample_bytree=0.8,       # Column sampling
    random_state=42             # Reproducibility
)
```

**Hyperparameter Guide**:
- `max_depth=6`: Shallow trees prevent memorization
- `learning_rate=0.1`: Standard value; lower (0.01) for more training time
- `subsample=0.8`: Uses 80% of data per tree (bagging effect)

---

#### Step 4: Training

```python
model.fit(
    X_train, y_train,
    eval_set=[(X_test, y_test)],  # Monitor test loss
    early_stopping_rounds=10,     # Stop if no improvement for 10 rounds
    verbose=10                    # Log every 10 rounds
)
```

**Early Stopping**: Prevents overfitting by stopping when validation loss plateaus.

---

#### Step 5: Evaluation

```python
from sklearn.metrics import classification_report

y_pred = model.predict(X_test)

print(classification_report(y_test, y_pred, 
                          target_names=LABEL_NAMES))
```

---

#### Step 6: Model Persistence

```python
import pickle

with open('models/xgb_classifier_5.pkl', 'wb') as f:
    pickle.dump(model, f)
```

**Alternative**: Use `model.save_model('model.json')` for XGBoost native format.

---

### Hyperparameter Tuning

**Manual Tuning**:
```bash
python scripts/train_with_labels.py \
    --max_depth 8 \
    --learning_rate 0.05 \
    --n_estimators 200
```

**Automated (Grid Search)**:
```python
from sklearn.model_selection import GridSearchCV

param_grid = {
    'max_depth': [4, 6, 8],
    'learning_rate': [0.01, 0.1, 0.2],
    'n_estimators': [100, 200, 300]
}

grid = GridSearchCV(
    xgb.XGBClassifier(),
    param_grid,
    cv=5,  # 5-fold cross-validation
    scoring='f1_weighted'
)

grid.fit(X_train, y_train)
best_model = grid.best_estimator_
```

**Warning**: Grid search is **computationally expensive** (27 combinations √ó 5 folds = 135 trainings).

---

### Model Interpretability

#### Feature Importance

```python
import matplotlib.pyplot as plt

importances = model.feature_importances_
features = X.columns

plt.barh(features, importances)
plt.xlabel('Importance (Gain)')
plt.title('Feature Importance Analysis')
plt.savefig('models/feature_importance.png')
```

**Example Output**:
```
max_stretch          ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 35%
polyphony_score      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 28%
avg_density          ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 18%
thirds_frequency     ‚ñà‚ñà‚ñà‚ñà 12%
velocity_variance    ‚ñà‚ñà 4%
...                  ‚ñà 3%
```

**Interpretation**: Physical reach (max_stretch) is the most predictive feature.

---

#### SHAP Values (Advanced)

```python
import shap

explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X_test)

# Visualize for a single prediction
shap.force_plot(explainer.expected_value[0], 
                shap_values[0][0], 
                X_test.iloc[0])
```

**Use Case**: Explain *why* a specific piece was classified as "Far Reach".

---

<a id="technology-stack"></a>
## üõ†Ô∏è Technology Stack & Library Rationale

We selected a robust, data-centric stack optimized for **precision**, **speed**, and **maintainability**.

### Core Dependencies

| Library | Version | Purpose | Why We Chose It |
|---------|---------|---------|-----------------|
| **numpy** | Latest | Numerical Computing | Foundation for all array operations. Fast vectorized math for feature calculations (mean, std, percentiles). Industry standard with C-optimized backends. |
| **pandas** | Latest | Data Manipulation | Handles large CSV datasets (`features_all.csv` with 10,000+ rows). Provides SQL-like operations (filtering, merging, grouping). Essential for train/test splitting and label alignment. |
| **pretty_midi** | Latest | MIDI Parsing | Higher-level abstraction than `mido`. Provides convenient access to notes, instruments, tempo, and timing. Built-in utilities for pitch/time conversions. |
| **scikit-learn** | Latest | ML Utilities | Industry-standard toolkit for data splitting (`train_test_split`), metrics (confusion matrices, classification reports), and preprocessing (normalization, encoding). Ensures reproducibility with `random_state`. |
| **xgboost** | Latest | Gradient Boosting | **The core model**. Chosen over deep learning (e.g., LSTM) because our features are **tabular/structured**. XGBoost excels at learning complex decision boundaries from engineered features. Provides feature importance for interpretability. Faster training than neural networks. |
| **torch** | Latest | Deep Learning (Future) | Currently minimal usage. Reserved for future enhancements (e.g., spectrogram-based models, audio-to-MIDI transcription). PyTorch chosen for its research-friendly API and dynamic computation graphs. |
| **openai** | Latest | LLM Integration (Future) | Placeholder for potential GPT-based feature explanations or automated labeling suggestions. Currently unused but in requirements for rapid prototyping. |
| **tqdm** | Latest | Progress Tracking | Provides progress bars for long-running operations (feature extraction, training). Critical for UX in CLI tools processing 10,000+ files. |
| **music21** | Latest | Music Theory Analysis | Computational music analysis toolkit. Used for advanced feature extraction (key detection, chord analysis, voice separation). Complements `pretty_midi` for high-level musical concepts. |
| **python-dotenv** | Latest | Environment Management | Loads API keys and config from `.env` files. Keeps secrets out of version control. Standard practice for production applications. |
| **matplotlib** | Latest | Visualization | Generates plots for model evaluation (confusion matrices, ROC curves). Essential for debugging feature distributions. |
| **seaborn** | Latest | Statistical Visualization | Higher-level plotting API built on matplotlib. Provides beautiful, publication-ready charts with minimal code (heatmaps, violin plots). |
| **flask** | Latest | Web Framework | Powers the manual labeling interface API. Lightweight WSGI framework perfect for internal tools. No overhead of Django; just RESTful endpoints. |
| **flask-cors** | Latest | CORS Handling | Enables the HTML frontend to communicate with the Flask backend. Critical for local development where frontend and backend run on different ports. |

### Why This Stack?

#### üéØ **Specialized for Tabular Data**
Unlike audio-based models (which require spectrograms and CNNs), we work with **symbolic music data** (MIDI). Our features are structured and numerical, making **tree-based models** (XGBoost) the optimal choice.

#### üî¨ **Research to Production**
- **Prototyping**: `pandas` + `scikit-learn` for rapid experimentation
- **Training**: `xgboost` for production-grade performance
- **Deployment**: `flask` for serving predictions via API

#### üìä **Explainability**
XGBoost provides:
- Feature importance scores (which metrics matter most?)
- Tree visualizations (how does the model decide?)
- SHAP value integration (future work)

This is crucial for **music education** where teachers need to understand *why* a piece is difficult.

---

<a id="future-improvements"></a>
## üöÄ Future Improvements & Roadmap

### Short-Term Enhancements (Next 3 Months)

#### 1Ô∏è‚É£ **Expanded Feature Set**
- **Hand Crossing Detection**: Count instances where left hand plays above right hand
- **Arpeggio Speed**: Measure notes per second in arpeggiated passages
- **Leap Frequency**: Track wide melodic jumps (>octave) requiring precise hand repositioning
- **Pedal Complexity**: Analyze sustain pedal usage patterns

**Implementation**: Add to `src/ml_engine/feature_extract.py`

---

#### 2Ô∏è‚É£ **Class Imbalance Handling**
**Problem**: "Multiple Voices" is underrepresented (7.3% of dataset)

**Solutions**:
- **SMOTE**: Synthetic Minority Over-sampling Technique
  ```python
  from imblearn.over_sampling import SMOTE
  
  smote = SMOTE(random_state=42)
  X_resampled, y_resampled = smote.fit_resample(X_train, y_train)
  ```
- **Class Weights**: Penalize misclassifications of rare classes more heavily
  ```python
  model = xgb.XGBClassifier(scale_pos_weight=3)  # 3x weight for minority class
  ```

---

#### 3Ô∏è‚É£ **Cross-Validation**
**Current**: Single 80/20 split  
**Improvement**: K-fold cross-validation for robust metrics

```python
from sklearn.model_selection import cross_val_score

scores = cross_val_score(model, X, y, cv=5, scoring='f1_weighted')
print(f"Mean F1: {scores.mean():.3f} ¬± {scores.std():.3f}")
```

---

### Medium-Term Goals (6-12 Months)

#### 4Ô∏è‚É£ **Deep Learning Experiments**
**Rationale**: While XGBoost is optimal for tabular data, neural networks might capture subtle patterns.

**Proposed Architecture**:
```python
import torch.nn as nn

class DifficultyClassifier(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(10, 64)  # 10 features ‚Üí 64 neurons
        self.fc2 = nn.Linear(64, 32)
        self.fc3 = nn.Linear(32, 5)   # 5 output classes
        
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        return torch.softmax(self.fc3(x), dim=1)
```

**Hypothesis**: Unlikely to outperform XGBoost, but worth benchmarking.

---

#### 5Ô∏è‚É£ **Audio-Based Features**
**Current Limitation**: MIDI lacks expressive nuances (dynamics, pedaling, tempo rubato)

**Solution**: Process actual audio recordings
- **Spectral Features**: MFCC, chroma, spectral centroid
- **Rhythm Analysis**: Beat tracking, tempo variations
- **Toolkit**: `librosa` for audio feature extraction

**Challenge**: Requires parallel dataset of MIDI + audio recordings.

---

#### 6Ô∏è‚É£ **Active Learning Loop**
**Concept**: Use model predictions to prioritize manual labeling

**Workflow**:
1. Train initial model on auto-labeled data
2. Run inference on unlabeled pool
3. Identify **low-confidence predictions** (e.g., probabilities close to 0.5)
4. Send these ambiguous cases to manual labeling
5. Retrain model with new labels

**Benefit**: Reduces manual labeling effort by 60%+

```python
predictions = model.predict_proba(X_unlabeled)
confidence = predictions.max(axis=1)

# Prioritize low-confidence samples
ambiguous = X_unlabeled[confidence < 0.7]
```

---

### Long-Term Vision (1-2 Years)

#### 7Ô∏è‚É£ **Real-Time Web API**
**Use Case**: Integrate with sheet music platforms (e.g., IMSLP, MuseScore)

**Tech Stack**:
- **FastAPI**: Modern Python web framework
- **Docker**: Containerized deployment
- **Redis**: Cache feature extractions

**API Endpoint**:
```bash
POST /api/predict
Content-Type: multipart/form-data

{
  "file": <uploaded .mid file>
}

Response:
{
  "difficulty": "Advanced Chords",
  "confidence": 0.89,
  "features": {
    "max_stretch": 28.0,
    "polyphony_score": 3.2
  },
  "recommendations": [
    "Requires large hand span",
    "Practice blocked chords before attempting"
  ]
}
```

---

#### 8Ô∏è‚É£ **Composer-Specific Models**
**Insight**: Difficulty is subjective; Chopin's "easy" is different from Bartok's.

**Approach**: Train separate models for different stylistic periods
- **Romantic Model** (Chopin, Liszt, Rachmaninoff)
- **Classical Model** (Mozart, Beethoven, Haydn)
- **Contemporary Model** (Ligeti, Glass, Adams)

**Implementation**: Multi-task learning with shared feature extraction, genre-specific classification heads.

---

#### 9Ô∏è‚É£ **Explainable AI Dashboard**
**Audience**: Music educators who need to justify difficulty ratings

**Features**:
- **SHAP Waterfall Plots**: Show contribution of each feature to final prediction
- **Comparative Analysis**: "This piece is harder than 78% of Chopin etudes because..."
- **Practice Suggestions**: "Focus on thirds passages (bars 12-24) which account for 40% of difficulty"

**Tech Stack**: React + D3.js for interactive visualizations

---

#### üîü **Mobile App**
**Vision**: Scan sheet music with phone camera ‚Üí Get instant difficulty rating

**Tech Stack**:
- **OMR**: Optical Music Recognition (convert images to MIDI)
- **React Native**: Cross-platform mobile app
- **TensorFlow Lite**: On-device inference

**Workflow**:
1. User photographs sheet music
2. OMR converts to MIDI
3. App extracts features locally
4. Pre-trained model classifies difficulty
5. Display results in <2 seconds

---

### üåü Visionary Projects (12-24 Months)

#### 1Ô∏è‚É£1Ô∏è‚É£ **Interactive Pedagogy Chatbot (RAG)**
**Goal**: A pedagogical assistant that explains *why* a piece is difficult.
- **Tech**: OpenAI GPT-4o + Vector Database (ChromaDB)
- **Feature**: User asks "How do I practice the thirds in bar 12?"
- **System**: Retrieves similar patterns from knowledge base and pedagogical texts (Czerny, Hanon) to give tailored advice.

#### 1Ô∏è‚É£2Ô∏è‚É£ **Real-Time Audio Analysis**
**Goal**: Move beyond MIDI to actual audio recordings.
- **Tech**: `librosa` + specialized audio-to-MIDI transcription models (e.g., `basic-pitch`, `demucs`)
- **Workflow**: User uploads MP3 -> System transcribes to MIDI -> Extracts features -> Classifies difficulty.
- **Status**: Requires high-fidelity transcription models.

#### 1Ô∏è‚É£3Ô∏è‚É£ **Crowdsourced Labeling Platform**
**Goal**: Gamified web app for global data collection.
- **Concept**: "Dualingo for Music Theory"
- **Mechanism**: Users earn points by correctly labeling difficulty or identifying techniques.
- **Impact**: Rapidly scale dataset from 10k to 100k+ validated samples.

---

### üõ†Ô∏è Technical Infrastructure Roadmap

#### 1Ô∏è‚É£4Ô∏è‚É£ **MLOps & CI/CD Pipeline**
**Goal**: Automate the model lifecycle for reliability and reproducibility.
- **GitHub Actions**:
  - `on: push` ‚Üí Run unit tests (`pytest`) and linting (`flake8`).
  - `on: release` ‚Üí Train model on full dataset -> Evaluate -> Deploy artifact.
- **DVC (Data Version Control)**: Version large MIDI datasets and model binaries alongside code.
- **Model Registry (MLflow)**: Track experiments, hyperparameters (learning rate, depth), and metrics (accuracy, F1) over time.

#### 1Ô∏è‚É£5Ô∏è‚É£ **RAG Engine Activation Plan**
**Current State**: `src/rag_engine` structure exists but is dormant.
**Activation Steps**:
1. **Knowledge Ingestion**:
   - Parse PDF pedagogy books (e.g., *Art of Piano Playing*).
   - Chunk text into semantic segments (e.g., "Thirds technique", "Octave relaxation").
2. **Vector Store Setup**:
   - Initialize local ChromaDB.
   - Embed text chunks using OpenAI `text-embedding-3-small`.
3. **Retrieval Pipeline**:
   - Connect `src/rag_engine/retriever.py` to the Vector Store.
   - Implement "Hybrid Search" (Keyword + Semantic) for precise music theory lookups.

---

<a id="contributing"></a>
## ü§ù Contributing

We welcome contributions from:
- üéπ Pianists and music educators
- üíª Machine learning engineers
- üìä Data scientists
- üéº Music theorists

### How to Contribute

#### 1Ô∏è‚É£ **Report Issues**
Found a bug or have a feature request?
- Open an issue on GitHub
- Use the template provided
- Tag appropriately (`bug`, `enhancement`, `documentation`)

#### 2Ô∏è‚É£ **Submit Pull Requests**
**Process**:
1. Fork the repository
2. Create a feature branch: `git checkout -b feature/new-feature`
3. Make changes with clear commit messages
4. Submit PR with detailed description

**Code Style**:
- Follow PEP 8 for Python
- Use type hints: `def extract_features(midi: PrettyMIDI) -> Dict[str, float]:`
- Document functions with docstrings

**Example**:
```python
def calculate_polyphony(midi: PrettyMIDI) -> float:
    """
    Calculate polyphony score based on voice independence.
    
    Args:
        midi: PrettyMIDI object containing note events
        
    Returns:
        Float score from 1.0 (monophonic) to 6.0 (complex polyphony)
        
    Example:
        >>> midi = pretty_midi.PrettyMIDI('bach_fugue.mid')
        >>> calculate_polyphony(midi)
        4.2
    """
    # Implementation...
```

#### 3Ô∏è‚É£ **Improve Documentation**
- Fix typos
- Add examples
- Translate to other languages
- Create tutorials

#### 4Ô∏è‚É£ **Share Datasets**
We're building a public dataset of labeled MIDI files. Contribute by:
- Uploading your MIDI collection (with licensing info)
- Verifying auto-generated labels
- Recording difficulty ratings from expert pianists

---

### Development Setup

```bash
# Clone your fork
git clone https://github.com/YOUR_USERNAME/Virtuoso-Architect.git
cd Virtuoso-Architect

# Create virtual environment
python -m venv venv
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

---

### Contribution Ideas

**Good First Issues**:
- Add progress bars to long-running scripts
- Improve error messages
- Write unit tests for feature extraction functions
- Create Jupyter notebooks with examples

**Advanced Projects**:
- Implement SHAP value visualizations
- Build a web demo using Streamlit
- Optimize feature extraction for 10x speedup
- Integrate with music21's corpus for automatic evaluation

---

<a id="license"></a>
## üìÑ License

This project is licensed under the **MIT License**.

```
MIT License

Copyright (c) 2024-2026 Virtuoso Architect Contributors

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
```

**What This Means**:
- ‚úÖ Use for commercial projects
- ‚úÖ Modify and distribute
- ‚úÖ Private use
- ‚ùå No warranty or liability

---

## üôè Acknowledgments

### Research Foundations
- **Music21**: Computational music analysis toolkit by MIT
- **Pretty MIDI**: MIDI processing library by Colin Raffel
- **XGBoost**: Gradient boosting framework by DMLC
- **GiantMIDI-Piano**: [Dataset Source](https://www.kaggle.com/datasets/pictureinthenoise/music-generation-with-giantmidi-piano) for training data

### Inspirations
- **Piano pedagogy research**: Suzuki Method, Faber Method
- **MIREX**: Music Information Retrieval Evaluation eXchange
- **IMSLP**: International Music Score Library Project

### Special Thanks
- Music educators who provided expert labels for validation
- Open-source contributors who improved documentation and code
- Community members who tested early versions

---

## üìû Contact & Support

### üêõ **Issues & Bugs**
Report on GitHub: [Issues Page](https://github.com/Mesutssmn/Virtuoso-Architect/issues)

### üí¨ **Discussions**
Join the conversation: [Discussions](https://github.com/Mesutssmn/Virtuoso-Architect/discussions)

### üìß **Email**
For private inquiries: [Contact via GitHub Profile](https://github.com/Mesutssmn)

### üåü **Stay Updated**
- Watch the repository for release notifications
- Star the project to support development
- Follow contributors for related projects

---

<div align="center">

**Made with ‚ù§Ô∏è by the Virtuoso Architect Team**

*Empowering musicians and educators through data-driven insights*

[‚¨Ü Back to Top](#-virtuoso-architect)

</div>
